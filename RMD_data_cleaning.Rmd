---
title: "Credit Card Fraud Data Acquisition and Cleaning Process"
author: "Ernest Kishkin"
date: "2026-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Cleaning Process

The dataset was downloaded from popular data environment website "Kaggle"

The following criteria was used in order to determine the usability of the data:

- The age of dataset = The last update was made on the dataset page on Kaggle 4 years ago, what makes this particular dataset only partially relevant for the recommendations for the future fraud detection policies.
- Desciptions of the variables = all columns are named accordingly with the values they represent, what makes data readable.
- Data source = unfortunately, data source was not disclosed, what adds to the partial trustworthiness of this dataset.

Nevertheless, for this particular data analysis project, this dataset was deemed suiting, as all the variables were clearly described and no sensitive cardholder information was being disclosed in the dataset.

It is important to determine the goals of analysis, thus, the following goals can be chosen:

- To calculate overall fraud rate.
- To determine the impact of different variables and conditions on the fraud rate.
- To visualize the impact of different conditions on the fraud rate.
- To make recommendations based on the results.

## 1. Loading the data

```{r}
credit_data <- read.csv("card_transdata.csv")
```

## 2. Checking for missing values
Checking for missing values is important in order to ensure that future calculation results (ex. total, mean, median etc.) are not omitted by null values in any collumns and rows.

```{r} 
colSums(is.na(credit_data))
```

As we can see, results show no null values in the data table.

## 3. Checking for duplicates and removing them
Duplicate values may distort calculations of the data and, thus, impact the results of the analysis. That being said, it is important to locate and remove duplicate values from the dataset.

First, we will be checking for the duplicate data with the following command:

```{r}
sum(duplicated(credit_data))
```

Returned answer shows, that there aren't any duplicate values. 
Just to ensure that there won't be any duplicate values anyway, the following command will be executed:

```{r} 
cleaned_creditdata <- unique(credit_data)
```

This command removed any possible duplicate values from the dataset.

Once the data cleaning process has been finished, we are ready to move onto the next step - Exploratory Data Analysis.